import streamlit as st
from dotenv import load_dotenv
import os
import time
import asyncio
import threading
from supabase import create_client, Client
from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader, TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from semantic_chunking import ChunkingManager
# --- Thay Ä‘á»•i 1: Import cÃ¡c lá»›p tá»« Google ---
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain.chains import ConversationalRetrievalChain
from langchain_community.vectorstores import SupabaseVectorStore
from langchain.memory import ConversationBufferMemory
import google.generativeai as genai # ThÃªm import nÃ y
from langchain.prompts import PromptTemplate
import cohere
from langchain_cohere import CohereRerank
from langchain.chains.combine_documents import create_stuff_documents_chain

# Import authentication modules
from auth import AuthManager, AuthUI



# ===== STREAMLIT PAGE CONFIG =====
st.set_page_config(
    page_title="ğŸ¤– AI Trá»£ Giáº£ng ToÃ¡n Tin",
    page_icon="ğŸ§ ",
    layout="wide",
    initial_sidebar_state="expanded",
    menu_items={
        'Get Help': 'https://github.com/yourusername/chatbot',
        'Report a bug': "https://github.com/yourusername/chatbot/issues",
        'About': "# AI Trá»£ Giáº£ng ToÃ¡n Tin\nPowered by Gemini & Supabase"
    }
)

# ===== CUSTOM CSS STYLING =====
st.markdown("""
<style>
/* Import Google Fonts */
@import url('https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap');

/* Global Styling */
.stApp {
    font-family: 'Inter', sans-serif;
    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
}

/* Main Container */
.main .block-container {
    padding-top: 2rem;
    padding-bottom: 2rem;
    max-width: 1200px;
}

/* Header Styling */
.main-header {
    text-align: center;
    padding: 2rem 0;
    background: rgba(255, 255, 255, 0.1);
    border-radius: 20px;
    backdrop-filter: blur(10px);
    border: 1px solid rgba(255, 255, 255, 0.2);
    margin-bottom: 2rem;
    box-shadow: 0 8px 32px rgba(0, 0, 0, 0.1);
}

.main-title {
    font-size: 3rem;
    font-weight: 700;
    background: linear-gradient(45deg, #FF6B6B, #4ECDC4, #45B7D1, #96CEB4);
    -webkit-background-clip: text;
    -webkit-text-fill-color: transparent;
    background-clip: text;
    margin-bottom: 0.5rem;
    animation: gradient 3s ease-in-out infinite;
}

.main-subtitle {
    font-size: 1.2rem;
    color: rgba(255, 255, 255, 0.8);
    font-weight: 400;
}

@keyframes gradient {
    0%, 100% { background-position: 0% 50%; }
    50% { background-position: 100% 50%; }
}

/* Sidebar Styling */
.css-1d391kg {
    background: linear-gradient(180deg, #2C3E50 0%, #34495E 100%);
}

.sidebar-header {
    background: linear-gradient(135deg, #FF6B6B, #4ECDC4);
    padding: 1.5rem;
    border-radius: 15px;
    margin-bottom: 1.5rem;
    text-align: center;
    color: white;
    font-weight: 600;
    box-shadow: 0 4px 15px rgba(0, 0, 0, 0.2);
}

/* File Uploader Styling */
.stFileUploader {
    background: rgba(255, 255, 255, 0.1);
    border-radius: 15px;
    padding: 1rem;
    border: 2px dashed rgba(255, 255, 255, 0.3);
    transition: all 0.3s ease;
}

.stFileUploader:hover {
    border-color: #4ECDC4;
    background: rgba(78, 205, 196, 0.1);
    transform: translateY(-2px);
}

/* Button Styling */
.stButton > button {
    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
    color: white;
    border: none;
    border-radius: 25px;
    padding: 0.75rem 2rem;
    font-weight: 600;
    font-size: 1rem;
    transition: all 0.3s ease;
    box-shadow: 0 4px 15px rgba(102, 126, 234, 0.4);
    width: 100%;
}

.stButton > button:hover {
    transform: translateY(-3px);
    box-shadow: 0 6px 20px rgba(102, 126, 234, 0.6);
    background: linear-gradient(135deg, #764ba2 0%, #667eea 100%);
}

/* Chat Messages */
.stChatMessage {
    background: rgba(255, 255, 255, 0.1);
    border-radius: 15px;
    padding: 1rem;
    margin: 0.5rem 0;
    backdrop-filter: blur(10px);
    border: 1px solid rgba(255, 255, 255, 0.2);
    animation: fadeIn 0.5s ease-in;
}

@keyframes fadeIn {
    from { opacity: 0; transform: translateY(10px); }
    to { opacity: 1; transform: translateY(0); }
}

/* User Message */
.stChatMessage[data-testid="user-message"] {
    background: linear-gradient(135deg, #FF6B6B, #FF8E53);
    margin-left: 20%;
}

/* Assistant Message */
.stChatMessage[data-testid="assistant-message"] {
    background: linear-gradient(135deg, #4ECDC4, #44A08D);
    margin-right: 20%;
}

/* Chat Input */
.stChatInput > div > div > input {
    background: rgba(255, 255, 255, 0.1);
    border: 2px solid rgba(255, 255, 255, 0.3);
    border-radius: 25px;
    color: white;
    font-size: 1rem;
    padding: 1rem 1.5rem;
    backdrop-filter: blur(10px);
}

.stChatInput > div > div > input:focus {
    border-color: #4ECDC4;
    box-shadow: 0 0 20px rgba(78, 205, 196, 0.3);
}

/* Success/Error Messages */
.stSuccess {
    background: linear-gradient(135deg, #56ab2f, #a8e6cf);
    border-radius: 15px;
    border: none;
    color: white;
    font-weight: 500;
}

.stError {
    background: linear-gradient(135deg, #ff416c, #ff4b2b);
    border-radius: 15px;
    border: none;
    color: white;
    font-weight: 500;
}

/* Spinner */
.stSpinner {
    text-align: center;
}

/* Stats Cards */
.stats-card {
    background: rgba(255, 255, 255, 0.1);
    padding: 1.5rem;
    border-radius: 15px;
    text-align: center;
    backdrop-filter: blur(10px);
    border: 1px solid rgba(255, 255, 255, 0.2);
    margin: 0.5rem;
    transition: transform 0.3s ease;
}

.stats-card:hover {
    transform: translateY(-5px);
}

.stats-number {
    font-size: 2rem;
    font-weight: 700;
    color: #4ECDC4;
}

.stats-label {
    font-size: 0.9rem;
    color: rgba(255, 255, 255, 0.7);
    margin-top: 0.5rem;
}

/* Responsive Design */
@media (max-width: 768px) {
    .main-title {
        font-size: 2rem;
    }
    
    .stChatMessage[data-testid="user-message"] {
        margin-left: 10%;
    }
    
    .stChatMessage[data-testid="assistant-message"] {
        margin-right: 10%;
    }
}
</style>
""", unsafe_allow_html=True)

# Táº£i cÃ¡c biáº¿n mÃ´i trÆ°á»ng tá»« file .env
load_dotenv()

def stream_parser(stream):
    """
    Láº¯ng nghe má»™t stream tá»« LangChain vÃ  chá»‰ yield (Ä‘áº©y ra) pháº§n ná»™i dung.
    """
    for chunk in stream:
        # Vá»›i document_chain, chunk lÃ  string trá»±c tiáº¿p
        if isinstance(chunk, str):
            yield chunk
        # Vá»›i ConversationalRetrievalChain, chunk cÃ³ key "answer"
        elif isinstance(chunk, dict) and "answer" in chunk:
            yield chunk["answer"]

def run_async_in_thread(async_func, *args):
    """
    Cháº¡y hÃ m async trong thread riÃªng Ä‘á»ƒ trÃ¡nh lá»—i event loop.
    """
    def run_in_thread():
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            return loop.run_until_complete(async_func(*args))
        finally:
            loop.close()
    
    thread = threading.Thread(target=run_in_thread)
    thread.start()
    thread.join()
    return thread.result if hasattr(thread, 'result') else None

def safe_stream_qa(qa_chain, prompt):
    """
    Wrapper an toÃ n cho streaming QA Ä‘á»ƒ trÃ¡nh lá»—i event loop.
    """
    try:
        # Thá»­ streaming trÆ°á»›c
        return qa_chain.stream(prompt)
    except Exception as e:
        if "event loop" in str(e).lower():
            # Náº¿u gáº·p lá»—i event loop, dÃ¹ng invoke thay tháº¿
            st.warning("Äang sá»­ dá»¥ng cháº¿ Ä‘á»™ Ä‘á»“ng bá»™ do giá»›i háº¡n ká»¹ thuáº­t...")
            result = qa_chain.invoke(prompt)
            # Táº¡o generator giáº£ Ä‘á»ƒ tÆ°Æ¡ng thÃ­ch vá»›i stream_parser
            def fake_stream():
                yield {"answer": result.get("answer", "")}
            return fake_stream()
        else:
            raise e

# --- Khá»Ÿi táº¡o vá»›i xá»­ lÃ½ event loop ---
def initialize_components():
    """Khá»Ÿi táº¡o cÃ¡c thÃ nh pháº§n vá»›i xá»­ lÃ½ event loop an toÃ n."""
    try:
        # Äáº£m báº£o cÃ³ event loop cho thread hiá»‡n táº¡i
        try:
            loop = asyncio.get_event_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
        
        google_api_key = os.getenv("GOOGLE_API_KEY")
        cohere_api_key = os.getenv("COHERE_API_KEY")

        if not cohere_api_key:
            st.error("Vui lÃ²ng thiáº¿t láº­p biáº¿n mÃ´i trÆ°á»ng COHERE_API_KEY trong file .env")
            st.stop()
        #cohere.configure(api_key=cohere_api_key)
        if not google_api_key:
            st.error("Vui lÃ²ng thiáº¿t láº­p biáº¿n mÃ´i trÆ°á»ng GOOGLE_API_KEY trong file .env")
            st.stop()
        genai.configure(api_key=google_api_key)

        supabase_url = os.getenv("SUPABASE_URL")
        supabase_key = os.getenv("SUPABASE_KEY")
        if not all([supabase_url, supabase_key]):
            st.error("Vui lÃ²ng thiáº¿t láº­p cÃ¡c biáº¿n mÃ´i trÆ°á»ng SUPABASE_URL, vÃ  SUPABASE_KEY trong file .env")
            st.stop()

        supabase: Client = create_client(supabase_url, supabase_key)

        # Khá»Ÿi táº¡o embeddings vá»›i xá»­ lÃ½ lá»—i
        embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
        
        vector_store = SupabaseVectorStore(
            client=supabase, 
            table_name="documents", 
            embedding=embeddings, 
            query_name="match_documents"
        )
        
        # Khá»Ÿi táº¡o LLM
        llm = ChatGoogleGenerativeAI(
            model="gemini-1.5-flash", 
            temperature=1.0, 
           # convert_system_message_to_human=True
        )
        
        return supabase, embeddings, vector_store, llm
        
    except Exception as e:
        st.error(f"Lá»—i khá»Ÿi táº¡o: {e}")
        st.stop()

# Khá»Ÿi táº¡o components
try:
    supabase, embeddings, vector_store, llm = initialize_components()
    
    # Khá»Ÿi táº¡o Authentication
    auth_manager = AuthManager(supabase)
    auth_ui = AuthUI(auth_manager)
    
    # Khá»Ÿi táº¡o Re-Ranker
    reranker = CohereRerank(
        model="rerank-multilingual-v3.0",
        top_n=5
    )
    
    # Khá»Ÿi táº¡o Semantic Chunking Manager
    chunking_manager = ChunkingManager(embeddings)
    # Thiáº¿t láº­p prompt
    prompt_template = ("""
                       Báº¡n lÃ  má»™t trá»£ giáº£ng AI chuyÃªn ngÃ nh cho sinh viÃªn Ä‘áº¡i há»c, thÃ¢n thiá»‡n vÃ  cá»±c ká»³ cáº©n tháº­n. Nhiá»‡m vá»¥ cá»§a báº¡n lÃ  giÃºp sinh viÃªn hiá»ƒu sÃ¢u cÃ¡c khÃ¡i niá»‡m, giáº£i bÃ i táº­p vÃ  Ã´n táº­p dá»±a trÃªn tÃ i liá»‡u há»c táº­p cá»§a há».

                       NHIá»†M Vá»¤: Dá»±a vÃ o dá»¯ liá»‡u trong tÃ i liá»‡u, tráº£ lá»i trá»±c tiáº¿p, náº¿u khÃ´ng biáº¿t thÃ¬ nÃ³i 'khÃ´ng cÃ³ trong dá»¯ liá»‡u'

                       HÆ¯á»šNG DáºªN:
                       1. Äá»c ká»¹ cÃ¢u há»i
                       2. Náº¿u thÃ´ng tin cÃ³ trong tÃ i liá»‡u: Tráº£ lá»i chi tiáº¿t, rÃµ rÃ ng
                       3. Náº¿u khÃ´ng cÃ³ thÃ´ng tin: NÃ³i ráº±ng "TÃ´i khÃ´ng tÃ¬m tháº¥y thÃ´ng tin nÃ y trong tÃ i liá»‡u Ä‘Ã£ cung cáº¥p"
                       4. Sá»­ dá»¥ng Markdown Ä‘á»ƒ format Ä‘áº¹p
                       
                       Ngá»¯ cáº£nh tÃ i liá»‡u:
                       {context}

                       CÃ¢u há»i: {question}
                       
                       Lá»‹ch sá»­ trÃ² chuyá»‡n:
                       {chat_history}

                       Tráº£ lá»i:""")
    
    QA_prompt = PromptTemplate.from_template(prompt_template)
                        
    # Thiáº¿t láº­p bá»™ nhá»› Ä‘á»ƒ lÆ°u trá»¯ lá»‹ch sá»­ cuá»™c trÃ² chuyá»‡n
    memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True, output_key="answer")

    # Chain nhá» Ä‘á»ƒ káº¿t há»£p tÃ i liá»‡u vÃ o prompt
    document_chain = create_stuff_documents_chain(llm, QA_prompt)

    # Chain lá»›n hÆ¡n
    #conversation_retrieval_chain = create_retrieval_chain(retriever=vector_store.as_retriever(search_kwargs={"k": 7}), document_chain=document_chain)

    # Táº¡o chuá»—i xá»­ lÃ½ há»™i thoáº¡i
    #qa = ConversationalRetrievalChain.from_llm(llm, retriever=vector_store.as_retriever(search_kwargs={"k": 7}), memory=memory, combine_docs_chain_kwargs={"prompt": QA_prompt})
    #qa = conversation_retrieval_chain
    retriever = vector_store.as_retriever(search_kwargs={"k": 5, "score_threshold": 0.5})
except Exception as e:
    st.error(f"Lá»—i khá»Ÿi táº¡o: {e}")
    st.stop()


def process_document(uploaded_file, chunking_strategy="adaptive"):
    """
    HÃ m xá»­ lÃ½ file Ä‘Æ°á»£c táº£i lÃªn: Ä‘á»c, cáº¯t nhá» thÃ´ng minh vÃ  lÆ°u embeddings vÃ o Supabase.
    """
    try:
        # Táº¡o thÆ° má»¥c temp náº¿u chÆ°a cÃ³
        temp_dir = "./temp_files"
        os.makedirs(temp_dir, exist_ok=True)
        
        temp_file_path = os.path.join(temp_dir, uploaded_file.name)
        
        with open(temp_file_path, "wb") as f:
            f.write(uploaded_file.getbuffer())

        if uploaded_file.type == "application/pdf":
            loader = PyPDFLoader(temp_file_path)
        elif uploaded_file.type == "application/vnd.openxmlformats-officedocument.wordprocessingml.document":
            loader = Docx2txtLoader(temp_file_path)
        else:
            loader = TextLoader(temp_file_path)
        
        documents = loader.load()
        for doc in documents:
            doc.page_content = doc.page_content.replace('\u0000', '')

        # Sá»­ dá»¥ng Semantic Chunking thay vÃ¬ basic chunking
        st.write(f"ğŸ§  **Äang Ã¡p dá»¥ng {chunking_strategy} chunking...**")
        docs, chunk_stats = chunking_manager.process_documents(documents, chunking_strategy)
        
        # Hiá»ƒn thá»‹ thá»‘ng kÃª chunking chi tiáº¿t
        st.write(f"ğŸ“Š **Thá»‘ng kÃª chunking:**")
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("Tá»•ng chunks", chunk_stats.get('total_chunks', 0))
        with col2:
            st.metric("Äá»™ dÃ i TB", f"{chunk_stats.get('avg_chunk_length', 0):.0f}")
        with col3:
            st.metric("Thá»i gian xá»­ lÃ½", f"{chunk_stats.get('processing_time', 0):.2f}s")
        
        # Hiá»ƒn thá»‹ phÃ¢n bá»‘ loáº¡i chunks
        chunk_types = chunk_stats.get('chunk_types', {})
        if chunk_types:
            st.write("ğŸ“‹ **PhÃ¢n bá»‘ loáº¡i chunks:**")
            for chunk_type, count in chunk_types.items():
                st.write(f"â€¢ {chunk_type}: {count} chunks")
        
        # Add metadata to documents
        for doc in docs:
            doc.metadata["source_file"] = uploaded_file.name
            doc.metadata["upload_time"] = time.time()
            doc.metadata["chunking_strategy"] = chunking_strategy
        
        vector_store.add_documents(docs)
        st.write(f"âœ… **ÄÃ£ lÆ°u {len(docs)} chunks vÃ o vector database**")

        os.remove(temp_file_path)
        return True, chunk_stats
    except Exception as e:
        st.error(f"Lá»—i khi xá»­ lÃ½ tÃ i liá»‡u: {e}")
        return False, {}

# ===== HELPER FUNCTIONS =====
def display_typing_animation():
    """Hiá»ƒn thá»‹ animation typing vá»›i emoji"""
    typing_placeholder = st.empty()
    for i in range(3):
        typing_placeholder.markdown(f"ğŸ¤– **Trá»£ lÃ½ AI Ä‘ang suy nghÄ©{'.' * (i + 1)}**")
        time.sleep(0.5)
    typing_placeholder.empty()

def get_file_stats():
    """Láº¥y thá»‘ng kÃª file Ä‘Ã£ upload"""
    if "uploaded_files_count" not in st.session_state:
        st.session_state.uploaded_files_count = 0
    if "total_messages" not in st.session_state:
        st.session_state.total_messages = len(st.session_state.get("messages", []))
    return st.session_state.uploaded_files_count, st.session_state.total_messages

# ===== AUTHENTICATION CHECK =====
if not auth_ui.render_auth_page():
    st.stop()

# ===== MAIN INTERFACE =====

# Beautiful Header with Animation
st.markdown("""
<div class="main-header">
    <h1 class="main-title">ğŸ§  AI Trá»£ Giáº£ng ToÃ¡n Tin</h1>
    <p class="main-subtitle">âœ¨ Powered by Gemini 2.5 Pro & Supabase Vector Store âœ¨</p>
    <div style="margin-top: 1rem;">
        <span style="background: rgba(255,255,255,0.2); padding: 0.5rem 1rem; border-radius: 20px; margin: 0 0.5rem; font-size: 0.9rem;">ğŸ“š Há»i Ä‘Ã¡p thÃ´ng minh</span>
        <span style="background: rgba(255,255,255,0.2); padding: 0.5rem 1rem; border-radius: 20px; margin: 0 0.5rem; font-size: 0.9rem;">ğŸš€ Streaming Response</span>
        <span style="background: rgba(255,255,255,0.2); padding: 0.5rem 1rem; border-radius: 20px; margin: 0 0.5rem; font-size: 0.9rem;">ğŸ¯ Context-Aware</span>
    </div>
</div>
""", unsafe_allow_html=True)

# Stats Dashboard
file_count, message_count = get_file_stats()
col1, col2, col3, col4 = st.columns(4)

with col1:
    st.markdown(f"""
    <div class="stats-card">
        <div class="stats-number">{file_count}</div>
        <div class="stats-label">ğŸ“„ TÃ i liá»‡u</div>
    </div>
    """, unsafe_allow_html=True)

with col2:
    st.markdown(f"""
    <div class="stats-card">
        <div class="stats-number">{message_count}</div>
        <div class="stats-label">ğŸ’¬ Tin nháº¯n</div>
    </div>
    """, unsafe_allow_html=True)

with col3:
    st.markdown("""
    <div class="stats-card">
        <div class="stats-number">ğŸ”¥</div>
        <div class="stats-label">âš¡ Gemini 2.5</div>
    </div>
    """, unsafe_allow_html=True)

with col4:
    st.markdown("""
    <div class="stats-card">
        <div class="stats-number">âœ¨</div>
        <div class="stats-label">ğŸ¨ Modern UI</div>
    </div>
    """, unsafe_allow_html=True)

# ===== ENHANCED SIDEBAR =====
with st.sidebar:
    # User Profile Section
    auth_ui.render_user_profile()
    
    st.markdown("---")
    
    # Sidebar Header
    st.markdown("""
    <div class="sidebar-header">
        <h2 style="margin: 0; font-size: 1.5rem;">ğŸ“š Quáº£n lÃ½ TÃ i liá»‡u</h2>
        <p style="margin: 0.5rem 0 0 0; opacity: 0.9;">Upload & xá»­ lÃ½ tÃ i liá»‡u há»c táº­p</p>
    </div>
    """, unsafe_allow_html=True)
    
    # File Upload Section - Admin Only
    if auth_manager.can_upload_documents():
        st.markdown("### ğŸ“ Táº£i lÃªn tÃ i liá»‡u")
        uploaded_files = st.file_uploader(
            "KÃ©o tháº£ hoáº·c chá»n file",
            type=["pdf", "docx", "txt"],
            accept_multiple_files=True,
            help="Há»— trá»£: PDF, DOCX, TXT. CÃ³ thá»ƒ chá»n nhiá»u file cÃ¹ng lÃºc."
        )
        
        # Chunking Strategy Selection
        st.markdown("### ğŸ§  Chiáº¿n lÆ°á»£c cáº¯t nhá» dá»¯ liá»‡u")
        strategies = chunking_manager.get_available_strategies()
        strategy_names = list(strategies.keys())
        strategy_descriptions = [strategies[key] for key in strategy_names]
        
        selected_strategy = st.selectbox(
            "Chá»n phÆ°Æ¡ng phÃ¡p chunking:",
            options=strategy_names,
            index=1,  # Default to 'adaptive'
            format_func=lambda x: f"{x.title()} - {strategies[x]}",
            help="Adaptive sáº½ tá»± Ä‘á»™ng chá»n phÆ°Æ¡ng phÃ¡p tá»‘t nháº¥t cho tá»«ng loáº¡i ná»™i dung"
        )
        
        # Show strategy description
        st.info(f"ğŸ“ **{selected_strategy.title()}**: {strategies[selected_strategy]}")
        
    else:
        st.markdown("### ğŸ”’ Táº£i lÃªn tÃ i liá»‡u")
        st.info("âš ï¸ Chá»‰ cÃ³ quáº£n trá»‹ viÃªn má»›i cÃ³ thá»ƒ táº£i lÃªn tÃ i liá»‡u.")
        uploaded_files = None
        selected_strategy = "adaptive"
    
    if uploaded_files:
        st.markdown(f"**ğŸ“Š ÄÃ£ chá»n {len(uploaded_files)} file:**")
        for file in uploaded_files:
            file_size = len(file.getvalue()) / 1024  # KB
            st.markdown(f"â€¢ `{file.name}` ({file_size:.1f} KB)")
        
        st.markdown("---")
        
        if st.button("ğŸš€ Xá»­ lÃ½ vÃ  Náº¡p kiáº¿n thá»©c", use_container_width=True):
            progress_bar = st.progress(0)
            status_text = st.empty()
            total_stats = {'total_chunks': 0, 'total_time': 0, 'strategies_used': []}
            
            for i, file in enumerate(uploaded_files):
                status_text.text(f"Äang xá»­ lÃ½: {file.name} vá»›i {selected_strategy} chunking")
                progress_bar.progress((i + 1) / len(uploaded_files))
                
                success, chunk_stats = process_document(file, selected_strategy)
                if success:
                    st.session_state.uploaded_files_count += 1
                    total_stats['total_chunks'] += chunk_stats.get('total_chunks', 0)
                    total_stats['total_time'] += chunk_stats.get('processing_time', 0)
                    total_stats['strategies_used'].append(chunk_stats.get('strategy_used', selected_strategy))
                    
            progress_bar.empty()
            status_text.empty()
            
            # Show summary statistics
            st.success(f"âœ… ÄÃ£ xá»­ lÃ½ thÃ nh cÃ´ng {len(uploaded_files)} tÃ i liá»‡u!")
            
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("ğŸ“„ Tá»•ng chunks", total_stats['total_chunks'])
            with col2:
                st.metric("â±ï¸ Tá»•ng thá»i gian", f"{total_stats['total_time']:.2f}s")
            with col3:
                st.metric("ğŸ§  Chiáº¿n lÆ°á»£c", selected_strategy.title())
            
            st.balloons()
    
    # Divider
    st.markdown("---")
    
    # Quick Actions
    st.markdown("### âš¡ Thao tÃ¡c nhanh")
    
    col1, col2 = st.columns(2)
    with col1:
        if st.button("ğŸ—‘ï¸ XÃ³a chat", use_container_width=True):
            st.session_state.messages = []
            st.rerun()
    
    with col2:
        if st.button("ğŸ”„ Reset", use_container_width=True):
            for key in list(st.session_state.keys()):
                del st.session_state[key]
            st.rerun()
    
    # Tips Section
    st.markdown("---")
    st.markdown("### ğŸ’¡ Máº¹o sá»­ dá»¥ng")
    st.markdown("""
    â€¢ ğŸ“ **Há»i cá»¥ thá»ƒ**: "Giáº£i thÃ­ch thuáº­t toÃ¡n Dijkstra"
    â€¢ ğŸ” **TÃ¬m kiáº¿m**: "TÃ¬m Ä‘á»‹nh nghÄ©a vá» Ä‘á»“ thá»‹"
    â€¢ ğŸ“Š **So sÃ¡nh**: "So sÃ¡nh BFS vÃ  DFS"
    â€¢ ğŸ§® **BÃ i táº­p**: "Cho vÃ­ dá»¥ vá» cÃ¢y nhá»‹ phÃ¢n"
    """)
    
    # Footer
    st.markdown("---")
    st.markdown("""
    <div style="text-align: center; opacity: 0.7; font-size: 0.8rem;">
        <p>ğŸ¤– Made with â¤ï¸ by Duy</p>
        <p>Powered by Streamlit & Gemini</p>
    </div>
    """, unsafe_allow_html=True)

# ===== ENHANCED CHAT INTERFACE =====

# Initialize session state
if "messages" not in st.session_state:
    st.session_state.messages = []

# Welcome message for first time users
if not st.session_state.messages:
    st.markdown("""
    <div style="text-align: center; padding: 2rem; background: rgba(255,255,255,0.1); border-radius: 15px; margin: 2rem 0; backdrop-filter: blur(10px);">
        <h3 style="color: #4ECDC4; margin-bottom: 1rem;">ğŸ‘‹ ChÃ o má»«ng Ä‘áº¿n vá»›i AI Trá»£ Giáº£ng!</h3>
        <p style="color: rgba(255,255,255,0.8); margin-bottom: 1rem;">TÃ´i lÃ  trá»£ lÃ½ AI chuyÃªn ngÃ nh ToÃ¡n Tin, sáºµn sÃ ng giÃºp báº¡n:</p>
        <div style="display: flex; justify-content: center; flex-wrap: wrap; gap: 1rem; margin-top: 1rem;">
            <span style="background: rgba(255,107,107,0.2); color: #FF6B6B; padding: 0.5rem 1rem; border-radius: 20px; font-size: 0.9rem;">ğŸ§® Giáº£i bÃ i táº­p</span>
            <span style="background: rgba(78,205,196,0.2); color: #4ECDC4; padding: 0.5rem 1rem; border-radius: 20px; font-size: 0.9rem;">ğŸ“š Giáº£i thÃ­ch lÃ½ thuyáº¿t</span>
            <span style="background: rgba(69,183,209,0.2); color: #45B7D1; padding: 0.5rem 1rem; border-radius: 20px; font-size: 0.9rem;">ğŸ” TÃ¬m kiáº¿m thÃ´ng tin</span>
        </div>
        <p style="color: rgba(255,255,255,0.6); font-size: 0.9rem; margin-top: 1rem;">ğŸ’¡ HÃ£y upload tÃ i liá»‡u vÃ  báº¯t Ä‘áº§u há»i Ä‘Ã¡p!</p>
    </div>
    """, unsafe_allow_html=True)

# Display chat messages with enhanced styling
for i, message in enumerate(st.session_state.messages):
    with st.chat_message(message["role"], avatar="ğŸ‘¤" if message["role"] == "user" else "ğŸ¤–"):
        if message["role"] == "user":
            st.markdown(f"**Báº¡n:** {message['content']}")
        else:
            st.markdown(message["content"])

# Enhanced chat input with suggestions
if prompt := st.chat_input("ğŸ’¬ HÃ£y Ä‘áº·t cÃ¢u há»i vá» tÃ i liá»‡u cá»§a báº¡n...", key="chat_input"):
    chat_history = memory.load_memory_variables({}).get("chat_history", [])

    # Add user message to history
    st.session_state.messages.append({"role": "user", "content": prompt})
    st.session_state.total_messages = len(st.session_state.messages)
    
    # Display user message
    with st.chat_message("user", avatar="ğŸ‘¤"):
        st.markdown(f"**Báº¡n:** {prompt}")

    # Display assistant response with enhanced streaming
    with st.chat_message("assistant", avatar="ğŸ¤–"):
        # Show typing indicator
        thinking_placeholder = st.empty()
        thinking_placeholder.markdown("ğŸ¤– **Äang phÃ¢n tÃ­ch tÃ i liá»‡u vÃ  táº¡o cÃ¢u tráº£ lá»i...** â³")
        
        try:
            # Debug: Show retrieval process
            st.write("ğŸ” **Debug - Äang tÃ¬m kiáº¿m tÃ i liá»‡u...**")
            retrieved_docs = retriever.invoke(prompt)
            st.write(f"ğŸ“„ **TÃ¬m tháº¥y {len(retrieved_docs)} tÃ i liá»‡u liÃªn quan**")
            
            # Show chunking strategy info for retrieved docs
            chunk_strategies = {}
            for doc in retrieved_docs:
                strategy = doc.metadata.get('chunking_strategy', 'unknown')
                chunk_strategies[strategy] = chunk_strategies.get(strategy, 0) + 1
            
            if chunk_strategies:
                strategy_info = ", ".join([f"{k}: {v}" for k, v in chunk_strategies.items()])
                st.write(f"ğŸ§  **Chunking strategies: {strategy_info}**")
            
            # Show retrieved documents for debugging
            # with st.expander("ğŸ“‹ Xem tÃ i liá»‡u Ä‘Æ°á»£c tÃ¬m tháº¥y"):
            #     for i, doc in enumerate(retrieved_docs):
            #         st.write(f"**TÃ i liá»‡u {i+1}:**")
            #         st.write(doc.page_content[:300] + "..." if len(doc.page_content) > 300 else doc.page_content)
            #         st.write("---")

            st.write("ğŸ¯ **Äang rerank tÃ i liá»‡u...**")
            reranked_docs = reranker.compress_documents(
                documents=retrieved_docs,
                query=prompt
            )
            st.write(f"âœ¨ **Sau rerank: {len(reranked_docs)} tÃ i liá»‡u tá»‘t nháº¥t**")
            
            # Show final context being sent to LLM
            # with st.expander("ğŸ“ Xem context cuá»‘i cÃ¹ng gá»­i cho AI"):
            #     context_text = "\n\n".join([doc.page_content for doc in reranked_docs])
            #     st.write(f"**Tá»•ng Ä‘á»™ dÃ i context:** {len(context_text)} kÃ½ tá»±")
            #     st.text_area("Context:", context_text, height=200)
            
            # Show reranked documents for debugging
            # with st.expander("ğŸ† Xem tÃ i liá»‡u sau rerank"):
            #     for i, doc in enumerate(reranked_docs):
            #         st.write(f"**Top {i+1}:**")
            #         st.write(doc.page_content[:300] + "..." if len(doc.page_content) > 300 else doc.page_content)
            #         st.write("---")

            st.write("ğŸ¤– **Äang táº¡o cÃ¢u tráº£ lá»i...**")
            
            # Stream the response using safe wrapper
            response_stream = document_chain.stream({
                "question": prompt,
                "context": reranked_docs,
                "chat_history": chat_history
            })
            thinking_placeholder.empty()
            
            # Display streaming response with better formatting
            full_response = st.write_stream(stream_parser(response_stream))
            
            # Add reaction buttons
            col1, col2, col3, col4 = st.columns([1, 1, 1, 6])
            with col1:
                if st.button("ğŸ‘", key=f"like_{len(st.session_state.messages)}"):
                    st.toast("Cáº£m Æ¡n pháº£n há»“i cá»§a báº¡n! ğŸ˜Š", icon="ğŸ‘")
            with col2:
                if st.button("ğŸ‘", key=f"dislike_{len(st.session_state.messages)}"):
                    st.toast("TÃ´i sáº½ cá»‘ gáº¯ng cáº£i thiá»‡n! ğŸ™", icon="ğŸ‘")
            with col3:
                if st.button("ğŸ”„", key=f"retry_{len(st.session_state.messages)}"):
                    st.rerun()
                    
        except Exception as e:
            thinking_placeholder.empty()
            st.error(f"âŒ CÃ³ lá»—i xáº£y ra: {str(e)}")
            full_response = "Xin lá»—i, tÃ´i gáº·p sá»± cá»‘ khi xá»­ lÃ½ cÃ¢u há»i cá»§a báº¡n. Vui lÃ²ng thá»­ láº¡i."
        
    # Add response to history and memory
    st.session_state.messages.append({"role": "assistant", "content": full_response})
    st.session_state.total_messages = len(st.session_state.messages)
    
    # Update memory with the conversation
    memory.save_context({"input": prompt}, {"answer": full_response})

# Quick question suggestions
if not st.session_state.messages or len(st.session_state.messages) < 2:
    st.markdown("### ğŸ’¡ CÃ¢u há»i gá»£i Ã½:")
    
    col1, col2 = st.columns(2)
    
    with col1:
        if st.button("ğŸ” TÃ¬m Ä‘á»‹nh nghÄ©a cÆ¡ báº£n", use_container_width=True):
            st.session_state.suggested_question = "HÃ£y giáº£i thÃ­ch cÃ¡c Ä‘á»‹nh nghÄ©a cÆ¡ báº£n trong tÃ i liá»‡u"
            st.rerun()
            
        if st.button("ğŸ“Š So sÃ¡nh cÃ¡c khÃ¡i niá»‡m", use_container_width=True):
            st.session_state.suggested_question = "So sÃ¡nh cÃ¡c khÃ¡i niá»‡m chÃ­nh trong tÃ i liá»‡u"
            st.rerun()
    
    with col2:
        if st.button("ğŸ§® Giáº£i thÃ­ch thuáº­t toÃ¡n", use_container_width=True):
            st.session_state.suggested_question = "Giáº£i thÃ­ch cÃ¡c thuáº­t toÃ¡n Ä‘Æ°á»£c Ä‘á» cáº­p trong tÃ i liá»‡u"
            st.rerun()
            
        if st.button("ğŸ“ TÃ³m táº¯t ná»™i dung", use_container_width=True):
            st.session_state.suggested_question = "TÃ³m táº¯t nhá»¯ng Ä‘iá»ƒm chÃ­nh trong tÃ i liá»‡u"
            st.rerun()
