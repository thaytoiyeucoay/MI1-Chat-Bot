import streamlit as st
from dotenv import load_dotenv
import os
import time
from supabase import create_client, Client
from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader, TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
# --- Thay ƒë·ªïi 1: Import c√°c l·ªõp t·ª´ Google ---
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain.chains import ConversationalRetrievalChain
from langchain_community.vectorstores import SupabaseVectorStore
from langchain.memory import ConversationBufferMemory
import google.generativeai as genai # Th√™m import n√†y
from langchain.prompts import PromptTemplate


# ===== STREAMLIT PAGE CONFIG =====
st.set_page_config(
    page_title="ü§ñ AI Tr·ª£ Gi·∫£ng To√°n Tin",
    page_icon="üß†",
    layout="wide",
    initial_sidebar_state="expanded",
    menu_items={
        'Get Help': 'https://github.com/yourusername/chatbot',
        'Report a bug': "https://github.com/yourusername/chatbot/issues",
        'About': "# AI Tr·ª£ Gi·∫£ng To√°n Tin\nPowered by Gemini & Supabase"
    }
)

# ===== CUSTOM CSS STYLING =====
st.markdown("""
<style>
/* Import Google Fonts */
@import url('https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap');

/* Global Styling */
.stApp {
    font-family: 'Inter', sans-serif;
    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
}

/* Main Container */
.main .block-container {
    padding-top: 2rem;
    padding-bottom: 2rem;
    max-width: 1200px;
}

/* Header Styling */
.main-header {
    text-align: center;
    padding: 2rem 0;
    background: rgba(255, 255, 255, 0.1);
    border-radius: 20px;
    backdrop-filter: blur(10px);
    border: 1px solid rgba(255, 255, 255, 0.2);
    margin-bottom: 2rem;
    box-shadow: 0 8px 32px rgba(0, 0, 0, 0.1);
}

.main-title {
    font-size: 3rem;
    font-weight: 700;
    background: linear-gradient(45deg, #FF6B6B, #4ECDC4, #45B7D1, #96CEB4);
    -webkit-background-clip: text;
    -webkit-text-fill-color: transparent;
    background-clip: text;
    margin-bottom: 0.5rem;
    animation: gradient 3s ease-in-out infinite;
}

.main-subtitle {
    font-size: 1.2rem;
    color: rgba(255, 255, 255, 0.8);
    font-weight: 400;
}

@keyframes gradient {
    0%, 100% { background-position: 0% 50%; }
    50% { background-position: 100% 50%; }
}

/* Sidebar Styling */
.css-1d391kg {
    background: linear-gradient(180deg, #2C3E50 0%, #34495E 100%);
}

.sidebar-header {
    background: linear-gradient(135deg, #FF6B6B, #4ECDC4);
    padding: 1.5rem;
    border-radius: 15px;
    margin-bottom: 1.5rem;
    text-align: center;
    color: white;
    font-weight: 600;
    box-shadow: 0 4px 15px rgba(0, 0, 0, 0.2);
}

/* File Uploader Styling */
.stFileUploader {
    background: rgba(255, 255, 255, 0.1);
    border-radius: 15px;
    padding: 1rem;
    border: 2px dashed rgba(255, 255, 255, 0.3);
    transition: all 0.3s ease;
}

.stFileUploader:hover {
    border-color: #4ECDC4;
    background: rgba(78, 205, 196, 0.1);
    transform: translateY(-2px);
}

/* Button Styling */
.stButton > button {
    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
    color: white;
    border: none;
    border-radius: 25px;
    padding: 0.75rem 2rem;
    font-weight: 600;
    font-size: 1rem;
    transition: all 0.3s ease;
    box-shadow: 0 4px 15px rgba(102, 126, 234, 0.4);
    width: 100%;
}

.stButton > button:hover {
    transform: translateY(-3px);
    box-shadow: 0 6px 20px rgba(102, 126, 234, 0.6);
    background: linear-gradient(135deg, #764ba2 0%, #667eea 100%);
}

/* Chat Messages */
.stChatMessage {
    background: rgba(255, 255, 255, 0.1);
    border-radius: 15px;
    padding: 1rem;
    margin: 0.5rem 0;
    backdrop-filter: blur(10px);
    border: 1px solid rgba(255, 255, 255, 0.2);
    animation: fadeIn 0.5s ease-in;
}

@keyframes fadeIn {
    from { opacity: 0; transform: translateY(10px); }
    to { opacity: 1; transform: translateY(0); }
}

/* User Message */
.stChatMessage[data-testid="user-message"] {
    background: linear-gradient(135deg, #FF6B6B, #FF8E53);
    margin-left: 20%;
}

/* Assistant Message */
.stChatMessage[data-testid="assistant-message"] {
    background: linear-gradient(135deg, #4ECDC4, #44A08D);
    margin-right: 20%;
}

/* Chat Input */
.stChatInput > div > div > input {
    background: rgba(255, 255, 255, 0.1);
    border: 2px solid rgba(255, 255, 255, 0.3);
    border-radius: 25px;
    color: white;
    font-size: 1rem;
    padding: 1rem 1.5rem;
    backdrop-filter: blur(10px);
}

.stChatInput > div > div > input:focus {
    border-color: #4ECDC4;
    box-shadow: 0 0 20px rgba(78, 205, 196, 0.3);
}

/* Success/Error Messages */
.stSuccess {
    background: linear-gradient(135deg, #56ab2f, #a8e6cf);
    border-radius: 15px;
    border: none;
    color: white;
    font-weight: 500;
}

.stError {
    background: linear-gradient(135deg, #ff416c, #ff4b2b);
    border-radius: 15px;
    border: none;
    color: white;
    font-weight: 500;
}

/* Spinner */
.stSpinner {
    text-align: center;
}

/* Stats Cards */
.stats-card {
    background: rgba(255, 255, 255, 0.1);
    padding: 1.5rem;
    border-radius: 15px;
    text-align: center;
    backdrop-filter: blur(10px);
    border: 1px solid rgba(255, 255, 255, 0.2);
    margin: 0.5rem;
    transition: transform 0.3s ease;
}

.stats-card:hover {
    transform: translateY(-5px);
}

.stats-number {
    font-size: 2rem;
    font-weight: 700;
    color: #4ECDC4;
}

.stats-label {
    font-size: 0.9rem;
    color: rgba(255, 255, 255, 0.7);
    margin-top: 0.5rem;
}

/* Responsive Design */
@media (max-width: 768px) {
    .main-title {
        font-size: 2rem;
    }
    
    .stChatMessage[data-testid="user-message"] {
        margin-left: 10%;
    }
    
    .stChatMessage[data-testid="assistant-message"] {
        margin-right: 10%;
    }
}
</style>
""", unsafe_allow_html=True)

# T·∫£i c√°c bi·∫øn m√¥i tr∆∞·ªùng t·ª´ file .env
load_dotenv()

def stream_parser(stream):
    """
    L·∫Øng nghe m·ªôt stream t·ª´ LangChain v√† ch·ªâ yield (ƒë·∫©y ra) ph·∫ßn n·ªôi dung c·ªßa "answer".
    """
    for chunk in stream:
        if "answer" in chunk:
            yield chunk["answer"]

# --- Thay ƒë·ªïi 2: L·∫•y Google API Key v√† c·∫•u h√¨nh ---
try:
    google_api_key = os.getenv("GOOGLE_API_KEY")
    if not google_api_key:
        st.error("Vui l√≤ng thi·∫øt l·∫≠p bi·∫øn m√¥i tr∆∞·ªùng GOOGLE_API_KEY trong file .env")
        st.stop()
    genai.configure(api_key=google_api_key) # C·∫•u h√¨nh API key cho th∆∞ vi·ªán Google

    supabase_url = os.getenv("SUPABASE_URL")
    supabase_key = os.getenv("SUPABASE_KEY")
    if not all([supabase_url, supabase_key]):
        st.error("Vui l√≤ng thi·∫øt l·∫≠p c√°c bi·∫øn m√¥i tr∆∞·ªùng SUPABASE_URL, v√† SUPABASE_KEY trong file .env")
        st.stop()

    supabase: Client = create_client(supabase_url, supabase_key)

    # --- Thay ƒë·ªïi 3: Kh·ªüi t·∫°o Google Embeddings ---
    # S·ª≠ d·ª•ng m√¥ h√¨nh embedding c·ªßa Google
    embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
    
    vector_store = SupabaseVectorStore(client=supabase, table_name="documents", embedding=embeddings, query_name = "match_documents")
    
    # --- Thay ƒë·ªïi 4: Kh·ªüi t·∫°o m√¥ h√¨nh Chat Gemini ---
    llm = ChatGoogleGenerativeAI(model="gemini-2.5-pro", temperature=0.3, convert_system_message_to_human=True)
    
    # Thi·∫øt l·∫≠p prompt
    prompt_template = ("""
                       B·∫°n l√† m·ªôt tr·ª£ gi·∫£ng AI chuy√™n ng√†nh To√°n Tin, th√¢n thi·ªán v√† c·ª±c k·ª≥ c·∫©n th·∫≠n. Nhi·ªám v·ª• c·ªßa b·∫°n l√† gi√∫p sinh vi√™n hi·ªÉu s√¢u c√°c kh√°i ni·ªám, gi·∫£i b√†i t·∫≠p v√† √¥n t·∫≠p d·ª±a tr√™n t√†i li·ªáu h·ªçc t·∫≠p c·ªßa h·ªç.

                       QUY T·∫ÆC V√ÄNG: C√¢u tr·∫£ l·ªùi c·ªßa b·∫°n B·∫ÆT BU·ªòC ph·∫£i d·ª±a HO√ÄN TO√ÄN v√†o N·ªòI DUNG c·ªßa "Ng·ªØ c·∫£nh t√†i li·ªáu" ƒë∆∞·ª£c cung c·∫•p d∆∞·ªõi ƒë√¢y. Kh√¥ng ƒë∆∞·ª£c b·ªãa ƒë·∫∑t hay s·ª≠ d·ª•ng ki·∫øn th·ª©c b√™n ngo√†i t√†i li·ªáu.

                       QUY TR√åNH TR·∫¢ L·ªúI:
                       1. ƒê·ªçc k·ªπ "C√¢u h·ªèi" v√† "Ng·ªØ c·∫£nh t√†i li·ªáu".
                        2. N·∫øu c√¢u h·ªèi c√≥ th·ªÉ ƒë∆∞·ª£c tr·∫£ l·ªùi t·ª´ ng·ªØ c·∫£nh, h√£y tr·∫£ l·ªùi m·ªôt c√°ch r√µ r√†ng v√† ƒëi th·∫≥ng v√†o v·∫•n ƒë·ªÅ.
                        - Khi gi·∫£i th√≠ch c√°c ƒë·ªãnh nghƒ©a, thu·∫≠t to√°n ho·∫∑c ƒë·ªãnh l√Ω (v√≠ d·ª•: 'ƒê·ªì th·ªã Euler', 'Thu·∫≠t to√°n Dijkstra'), h√£y c·ªë g·∫Øng tr√¨nh b√†y theo c·∫•u tr√∫c: ƒê·ªãnh nghƒ©a -> T√≠nh ch·∫•t/C√°c b∆∞·ªõc -> V√≠ d·ª• (n·∫øu c√≥ trong t√†i li·ªáu).
                        - S·ª≠ d·ª•ng Markdown ƒë·ªÉ ƒë·ªãnh d·∫°ng c√¢u tr·∫£ l·ªùi cho d·ªÖ ƒë·ªçc: **in ƒë·∫≠m** c√°c thu·∫≠t ng·ªØ quan tr·ªçng, d√πng danh s√°ch (list) cho c√°c b∆∞·ªõc ho·∫∑c t√≠nh ch·∫•t.
                        3. N·∫øu c√¢u h·ªèi kh√¥ng th·ªÉ ƒë∆∞·ª£c tr·∫£ l·ªùi t·ª´ ng·ªØ c·∫£nh, h√£y tr·∫£ l·ªùi m·ªôt c√°ch l·ªãch s·ª± r·∫±ng: "T√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin v·ªÅ ch·ªß ƒë·ªÅ n√†y trong c√°c t√†i li·ªáu b·∫°n ƒë√£ cung c·∫•p."
                        4. C√¢u tr·∫£ l·ªùi c·∫ßn c√≥ ƒë·ªãnh d·∫°ng ƒë·∫πp, d·ªÖ nh√¨n, markdown.
                        ---
                        Ng·ªØ c·∫£nh t√†i li·ªáu:
                        {context}
                        ---

                        C√¢u h·ªèi: {question}

                        C√¢u tr·∫£ l·ªùi c·ªßa Tr·ª£ gi·∫£ng AI: """)
    
    QA_prompt = PromptTemplate.from_template(prompt_template)
                        
    # Thi·∫øt l·∫≠p b·ªô nh·ªõ ƒë·ªÉ l∆∞u tr·ªØ l·ªãch s·ª≠ cu·ªôc tr√≤ chuy·ªán
    memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

    # T·∫°o chu·ªói x·ª≠ l√Ω h·ªôi tho·∫°i
    qa = ConversationalRetrievalChain.from_llm(llm, retriever=vector_store.as_retriever(), memory=memory, combine_docs_chain_kwargs={"prompt": QA_prompt})

except Exception as e:
    st.error(f"L·ªói kh·ªüi t·∫°o: {e}")
    st.stop()


def process_document(uploaded_file):
    """
    H√†m x·ª≠ l√Ω file ƒë∆∞·ª£c t·∫£i l√™n: ƒë·ªçc, c·∫Øt nh·ªè v√† l∆∞u embeddings v√†o Supabase.
    """
    try:
        # T·∫°o th∆∞ m·ª•c temp n·∫øu ch∆∞a c√≥
        temp_dir = "./temp_files"
        os.makedirs(temp_dir, exist_ok=True)
        
        temp_file_path = os.path.join(temp_dir, uploaded_file.name)
        
        with open(temp_file_path, "wb") as f:
            f.write(uploaded_file.getbuffer())

        if uploaded_file.type == "application/pdf":
            loader = PyPDFLoader(temp_file_path)
        elif uploaded_file.type == "application/vnd.openxmlformats-officedocument.wordprocessingml.document":
            loader = Docx2txtLoader(temp_file_path)
        else:
            loader = TextLoader(temp_file_path)
        
        documents = loader.load()
        for doc in documents:
            doc.page_content = doc.page_content.replace('\u0000', '')

        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        docs = text_splitter.split_documents(documents)
        
        vector_store.add_documents(docs)

        os.remove(temp_file_path)
        return True
    except Exception as e:
        st.error(f"L·ªói khi x·ª≠ l√Ω t√†i li·ªáu: {e}")
        return False

# ===== HELPER FUNCTIONS =====
def display_typing_animation():
    """Hi·ªÉn th·ªã animation typing v·ªõi emoji"""
    typing_placeholder = st.empty()
    for i in range(3):
        typing_placeholder.markdown(f"ü§ñ **Tr·ª£ l√Ω AI ƒëang suy nghƒ©{'.' * (i + 1)}**")
        time.sleep(0.5)
    typing_placeholder.empty()

def get_file_stats():
    """L·∫•y th·ªëng k√™ file ƒë√£ upload"""
    if "uploaded_files_count" not in st.session_state:
        st.session_state.uploaded_files_count = 0
    if "total_messages" not in st.session_state:
        st.session_state.total_messages = len(st.session_state.get("messages", []))
    return st.session_state.uploaded_files_count, st.session_state.total_messages

# ===== MAIN INTERFACE =====

# Beautiful Header with Animation
st.markdown("""
<div class="main-header">
    <h1 class="main-title">üß† AI Tr·ª£ Gi·∫£ng To√°n Tin</h1>
    <p class="main-subtitle">‚ú® Powered by Gemini 2.5 Pro & Supabase Vector Store ‚ú®</p>
    <div style="margin-top: 1rem;">
        <span style="background: rgba(255,255,255,0.2); padding: 0.5rem 1rem; border-radius: 20px; margin: 0 0.5rem; font-size: 0.9rem;">üìö H·ªèi ƒë√°p th√¥ng minh</span>
        <span style="background: rgba(255,255,255,0.2); padding: 0.5rem 1rem; border-radius: 20px; margin: 0 0.5rem; font-size: 0.9rem;">üöÄ Streaming Response</span>
        <span style="background: rgba(255,255,255,0.2); padding: 0.5rem 1rem; border-radius: 20px; margin: 0 0.5rem; font-size: 0.9rem;">üéØ Context-Aware</span>
    </div>
</div>
""", unsafe_allow_html=True)

# Stats Dashboard
file_count, message_count = get_file_stats()
col1, col2, col3, col4 = st.columns(4)

with col1:
    st.markdown(f"""
    <div class="stats-card">
        <div class="stats-number">{file_count}</div>
        <div class="stats-label">üìÑ T√†i li·ªáu</div>
    </div>
    """, unsafe_allow_html=True)

with col2:
    st.markdown(f"""
    <div class="stats-card">
        <div class="stats-number">{message_count}</div>
        <div class="stats-label">üí¨ Tin nh·∫Øn</div>
    </div>
    """, unsafe_allow_html=True)

with col3:
    st.markdown("""
    <div class="stats-card">
        <div class="stats-number">üî•</div>
        <div class="stats-label">‚ö° Gemini 2.5</div>
    </div>
    """, unsafe_allow_html=True)

with col4:
    st.markdown("""
    <div class="stats-card">
        <div class="stats-number">‚ú®</div>
        <div class="stats-label">üé® Modern UI</div>
    </div>
    """, unsafe_allow_html=True)

# ===== ENHANCED SIDEBAR =====
with st.sidebar:
    # Sidebar Header
    st.markdown("""
    <div class="sidebar-header">
        <h2 style="margin: 0; font-size: 1.5rem;">üìö Qu·∫£n l√Ω T√†i li·ªáu</h2>
        <p style="margin: 0.5rem 0 0 0; opacity: 0.9;">Upload & x·ª≠ l√Ω t√†i li·ªáu h·ªçc t·∫≠p</p>
    </div>
    """, unsafe_allow_html=True)
    
    # File Upload Section
    st.markdown("### üìÅ T·∫£i l√™n t√†i li·ªáu")
    uploaded_files = st.file_uploader(
        "K√©o th·∫£ ho·∫∑c ch·ªçn file",
        type=["pdf", "docx", "txt"],
        accept_multiple_files=True,
        help="H·ªó tr·ª£: PDF, DOCX, TXT. C√≥ th·ªÉ ch·ªçn nhi·ªÅu file c√πng l√∫c."
    )
    
    if uploaded_files:
        st.markdown(f"**üìä ƒê√£ ch·ªçn {len(uploaded_files)} file:**")
        for file in uploaded_files:
            file_size = len(file.getvalue()) / 1024  # KB
            st.markdown(f"‚Ä¢ `{file.name}` ({file_size:.1f} KB)")
        
        st.markdown("---")
        
        if st.button("üöÄ X·ª≠ l√Ω v√† N·∫°p ki·∫øn th·ª©c", use_container_width=True):
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            for i, file in enumerate(uploaded_files):
                status_text.text(f"ƒêang x·ª≠ l√Ω: {file.name}")
                progress_bar.progress((i + 1) / len(uploaded_files))
                
                success = process_document(file)
                if success:
                    st.session_state.uploaded_files_count += 1
                    
            progress_bar.empty()
            status_text.empty()
            st.success(f"‚úÖ ƒê√£ x·ª≠ l√Ω th√†nh c√¥ng {len(uploaded_files)} t√†i li·ªáu!")
            st.balloons()
    
    # Divider
    st.markdown("---")
    
    # Quick Actions
    st.markdown("### ‚ö° Thao t√°c nhanh")
    
    col1, col2 = st.columns(2)
    with col1:
        if st.button("üóëÔ∏è X√≥a chat", use_container_width=True):
            st.session_state.messages = []
            st.rerun()
    
    with col2:
        if st.button("üîÑ Reset", use_container_width=True):
            for key in list(st.session_state.keys()):
                del st.session_state[key]
            st.rerun()
    
    # Tips Section
    st.markdown("---")
    st.markdown("### üí° M·∫πo s·ª≠ d·ª•ng")
    st.markdown("""
    ‚Ä¢ üìù **H·ªèi c·ª• th·ªÉ**: "Gi·∫£i th√≠ch thu·∫≠t to√°n Dijkstra"
    ‚Ä¢ üîç **T√¨m ki·∫øm**: "T√¨m ƒë·ªãnh nghƒ©a v·ªÅ ƒë·ªì th·ªã"
    ‚Ä¢ üìä **So s√°nh**: "So s√°nh BFS v√† DFS"
    ‚Ä¢ üßÆ **B√†i t·∫≠p**: "Cho v√≠ d·ª• v·ªÅ c√¢y nh·ªã ph√¢n"
    """)
    
    # Footer
    st.markdown("---")
    st.markdown("""
    <div style="text-align: center; opacity: 0.7; font-size: 0.8rem;">
        <p>ü§ñ Made with ‚ù§Ô∏è by Duy</p>
        <p>Powered by Streamlit & Gemini</p>
    </div>
    """, unsafe_allow_html=True)

# ===== ENHANCED CHAT INTERFACE =====

# Initialize session state
if "messages" not in st.session_state:
    st.session_state.messages = []

# Welcome message for first time users
if not st.session_state.messages:
    st.markdown("""
    <div style="text-align: center; padding: 2rem; background: rgba(255,255,255,0.1); border-radius: 15px; margin: 2rem 0; backdrop-filter: blur(10px);">
        <h3 style="color: #4ECDC4; margin-bottom: 1rem;">üëã Ch√†o m·ª´ng ƒë·∫øn v·ªõi AI Tr·ª£ Gi·∫£ng!</h3>
        <p style="color: rgba(255,255,255,0.8); margin-bottom: 1rem;">T√¥i l√† tr·ª£ l√Ω AI chuy√™n ng√†nh To√°n Tin, s·∫µn s√†ng gi√∫p b·∫°n:</p>
        <div style="display: flex; justify-content: center; flex-wrap: wrap; gap: 1rem; margin-top: 1rem;">
            <span style="background: rgba(255,107,107,0.2); color: #FF6B6B; padding: 0.5rem 1rem; border-radius: 20px; font-size: 0.9rem;">üßÆ Gi·∫£i b√†i t·∫≠p</span>
            <span style="background: rgba(78,205,196,0.2); color: #4ECDC4; padding: 0.5rem 1rem; border-radius: 20px; font-size: 0.9rem;">üìö Gi·∫£i th√≠ch l√Ω thuy·∫øt</span>
            <span style="background: rgba(69,183,209,0.2); color: #45B7D1; padding: 0.5rem 1rem; border-radius: 20px; font-size: 0.9rem;">üîç T√¨m ki·∫øm th√¥ng tin</span>
        </div>
        <p style="color: rgba(255,255,255,0.6); font-size: 0.9rem; margin-top: 1rem;">üí° H√£y upload t√†i li·ªáu v√† b·∫Øt ƒë·∫ßu h·ªèi ƒë√°p!</p>
    </div>
    """, unsafe_allow_html=True)

# Display chat messages with enhanced styling
for i, message in enumerate(st.session_state.messages):
    with st.chat_message(message["role"], avatar="üë§" if message["role"] == "user" else "ü§ñ"):
        if message["role"] == "user":
            st.markdown(f"**B·∫°n:** {message['content']}")
        else:
            st.markdown(message["content"])

# Enhanced chat input with suggestions
if prompt := st.chat_input("üí¨ H√£y ƒë·∫∑t c√¢u h·ªèi v·ªÅ t√†i li·ªáu c·ªßa b·∫°n...", key="chat_input"):
    # Add user message to history
    st.session_state.messages.append({"role": "user", "content": prompt})
    st.session_state.total_messages = len(st.session_state.messages)
    
    # Display user message
    with st.chat_message("user", avatar="üë§"):
        st.markdown(f"**B·∫°n:** {prompt}")

    # Display assistant response with enhanced streaming
    with st.chat_message("assistant", avatar="ü§ñ"):
        # Show typing indicator
        thinking_placeholder = st.empty()
        thinking_placeholder.markdown("ü§ñ **ƒêang ph√¢n t√≠ch t√†i li·ªáu v√† t·∫°o c√¢u tr·∫£ l·ªùi...** ‚è≥")
        
        try:
            # Stream the response
            raw_stream = qa.stream(prompt)
            thinking_placeholder.empty()
            
            # Display streaming response with better formatting
            full_response = st.write_stream(stream_parser(raw_stream))
            
            # Add reaction buttons
            col1, col2, col3, col4 = st.columns([1, 1, 1, 6])
            with col1:
                if st.button("üëç", key=f"like_{len(st.session_state.messages)}"):
                    st.toast("C·∫£m ∆°n ph·∫£n h·ªìi c·ªßa b·∫°n! üòä", icon="üëç")
            with col2:
                if st.button("üëé", key=f"dislike_{len(st.session_state.messages)}"):
                    st.toast("T√¥i s·∫Ω c·ªë g·∫Øng c·∫£i thi·ªán! üôè", icon="üëé")
            with col3:
                if st.button("üîÑ", key=f"retry_{len(st.session_state.messages)}"):
                    st.rerun()
                    
        except Exception as e:
            thinking_placeholder.empty()
            st.error(f"‚ùå C√≥ l·ªói x·∫£y ra: {str(e)}")
            full_response = "Xin l·ªói, t√¥i g·∫∑p s·ª± c·ªë khi x·ª≠ l√Ω c√¢u h·ªèi c·ªßa b·∫°n. Vui l√≤ng th·ª≠ l·∫°i."
        
    # Add response to history
    st.session_state.messages.append({"role": "assistant", "content": full_response})
    st.session_state.total_messages = len(st.session_state.messages)

# Quick question suggestions
if not st.session_state.messages or len(st.session_state.messages) < 2:
    st.markdown("### üí° C√¢u h·ªèi g·ª£i √Ω:")
    
    col1, col2 = st.columns(2)
    
    with col1:
        if st.button("üîç T√¨m ƒë·ªãnh nghƒ©a c∆° b·∫£n", use_container_width=True):
            st.session_state.suggested_question = "H√£y gi·∫£i th√≠ch c√°c ƒë·ªãnh nghƒ©a c∆° b·∫£n trong t√†i li·ªáu"
            st.rerun()
            
        if st.button("üìä So s√°nh c√°c kh√°i ni·ªám", use_container_width=True):
            st.session_state.suggested_question = "So s√°nh c√°c kh√°i ni·ªám ch√≠nh trong t√†i li·ªáu"
            st.rerun()
    
    with col2:
        if st.button("üßÆ Gi·∫£i th√≠ch thu·∫≠t to√°n", use_container_width=True):
            st.session_state.suggested_question = "Gi·∫£i th√≠ch c√°c thu·∫≠t to√°n ƒë∆∞·ª£c ƒë·ªÅ c·∫≠p trong t√†i li·ªáu"
            st.rerun()
            
        if st.button("üìù T√≥m t·∫Øt n·ªôi dung", use_container_width=True):
            st.session_state.suggested_question = "T√≥m t·∫Øt nh·ªØng ƒëi·ªÉm ch√≠nh trong t√†i li·ªáu"
            st.rerun()
