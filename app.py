import streamlit as st
from dotenv import load_dotenv
import os
import time
import asyncio
import threading
from supabase import create_client, Client
from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader, TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from semantic_chunking import ChunkingManager
# --- Thay ƒë·ªïi 1: Import c√°c l·ªõp t·ª´ Google ---
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain.chains import ConversationalRetrievalChain
from langchain_community.vectorstores import SupabaseVectorStore
from langchain.memory import ConversationBufferMemory
import google.generativeai as genai # Th√™m import n√†y
from langchain.prompts import PromptTemplate
import cohere
from langchain_cohere import CohereRerank
from langchain.chains.combine_documents import create_stuff_documents_chain

# Import authentication modules
from auth import AuthManager, AuthUI



# ===== STREAMLIT PAGE CONFIG =====
st.set_page_config(
    page_title="ü§ñ AI Tr·ª£ Gi·∫£ng To√°n Tin",
    page_icon="üß†",
    layout="wide",
    initial_sidebar_state="expanded",
    menu_items={
        'Get Help': 'https://github.com/yourusername/chatbot',
        'Report a bug': "https://github.com/yourusername/chatbot/issues",
        'About': "# AI Tr·ª£ Gi·∫£ng To√°n Tin\nPowered by Gemini & Supabase"
    }
)

# ===== CUSTOM CSS STYLING =====
st.markdown("""
<style>
/* Import Google Fonts */
@import url('https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap');

/* Global Styling */
.stApp {
    font-family: 'Inter', sans-serif;
    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
}

/* Main Container */
.main .block-container {
    padding-top: 2rem;
    padding-bottom: 2rem;
    max-width: 1200px;
}

/* Header Styling */
.main-header {
    text-align: center;
    padding: 2rem 0;
    background: rgba(255, 255, 255, 0.1);
    border-radius: 20px;
    backdrop-filter: blur(10px);
    border: 1px solid rgba(255, 255, 255, 0.2);
    margin-bottom: 2rem;
    box-shadow: 0 8px 32px rgba(0, 0, 0, 0.1);
}

.main-title {
    font-size: 3rem;
    font-weight: 700;
    background: linear-gradient(45deg, #FF6B6B, #4ECDC4, #45B7D1, #96CEB4);
    -webkit-background-clip: text;
    -webkit-text-fill-color: transparent;
    background-clip: text;
    margin-bottom: 0.5rem;
    animation: gradient 3s ease-in-out infinite;
}

.main-subtitle {
    font-size: 1.2rem;
    color: rgba(255, 255, 255, 0.8);
    font-weight: 400;
}

@keyframes gradient {
    0%, 100% { background-position: 0% 50%; }
    50% { background-position: 100% 50%; }
}

/* Sidebar Styling */
.css-1d391kg {
    background: linear-gradient(180deg, #2C3E50 0%, #34495E 100%);
}

.sidebar-header {
    background: linear-gradient(135deg, #FF6B6B, #4ECDC4);
    padding: 1.5rem;
    border-radius: 15px;
    margin-bottom: 1.5rem;
    text-align: center;
    color: white;
    font-weight: 600;
    box-shadow: 0 4px 15px rgba(0, 0, 0, 0.2);
}

/* File Uploader Styling */
.stFileUploader {
    background: rgba(255, 255, 255, 0.1);
    border-radius: 15px;
    padding: 1rem;
    border: 2px dashed rgba(255, 255, 255, 0.3);
    transition: all 0.3s ease;
}

.stFileUploader:hover {
    border-color: #4ECDC4;
    background: rgba(78, 205, 196, 0.1);
    transform: translateY(-2px);
}

/* Button Styling */
.stButton > button {
    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
    color: white;
    border: none;
    border-radius: 25px;
    padding: 0.75rem 2rem;
    font-weight: 600;
    font-size: 1rem;
    transition: all 0.3s ease;
    box-shadow: 0 4px 15px rgba(102, 126, 234, 0.4);
    width: 100%;
}

.stButton > button:hover {
    transform: translateY(-3px);
    box-shadow: 0 6px 20px rgba(102, 126, 234, 0.6);
    background: linear-gradient(135deg, #764ba2 0%, #667eea 100%);
}

/* Chat Messages */
.stChatMessage {
    background: rgba(255, 255, 255, 0.1);
    border-radius: 15px;
    padding: 1rem;
    margin: 0.5rem 0;
    backdrop-filter: blur(10px);
    border: 1px solid rgba(255, 255, 255, 0.2);
    animation: fadeIn 0.5s ease-in;
}

@keyframes fadeIn {
    from { opacity: 0; transform: translateY(10px); }
    to { opacity: 1; transform: translateY(0); }
}

/* User Message */
.stChatMessage[data-testid="user-message"] {
    background: linear-gradient(135deg, #FF6B6B, #FF8E53);
    margin-left: 20%;
}

/* Assistant Message */
.stChatMessage[data-testid="assistant-message"] {
    background: linear-gradient(135deg, #4ECDC4, #44A08D);
    margin-right: 20%;
}

/* Chat Input */
.stChatInput > div > div > input {
    background: rgba(255, 255, 255, 0.1);
    border: 2px solid rgba(255, 255, 255, 0.3);
    border-radius: 25px;
    color: white;
    font-size: 1rem;
    padding: 1rem 1.5rem;
    backdrop-filter: blur(10px);
}

.stChatInput > div > div > input:focus {
    border-color: #4ECDC4;
    box-shadow: 0 0 20px rgba(78, 205, 196, 0.3);
}

/* Success/Error Messages */
.stSuccess {
    background: linear-gradient(135deg, #56ab2f, #a8e6cf);
    border-radius: 15px;
    border: none;
    color: white;
    font-weight: 500;
}

.stError {
    background: linear-gradient(135deg, #ff416c, #ff4b2b);
    border-radius: 15px;
    border: none;
    color: white;
    font-weight: 500;
}

/* Spinner */
.stSpinner {
    text-align: center;
}

/* Stats Cards */
.stats-card {
    background: rgba(255, 255, 255, 0.1);
    padding: 1.5rem;
    border-radius: 15px;
    text-align: center;
    backdrop-filter: blur(10px);
    border: 1px solid rgba(255, 255, 255, 0.2);
    margin: 0.5rem;
    transition: transform 0.3s ease;
}

.stats-card:hover {
    transform: translateY(-5px);
}

.stats-number {
    font-size: 2rem;
    font-weight: 700;
    color: #4ECDC4;
}

.stats-label {
    font-size: 0.9rem;
    color: rgba(255, 255, 255, 0.7);
    margin-top: 0.5rem;
}

/* Responsive Design */
@media (max-width: 768px) {
    .main-title {
        font-size: 2rem;
    }
    
    .stChatMessage[data-testid="user-message"] {
        margin-left: 10%;
    }
    
    .stChatMessage[data-testid="assistant-message"] {
        margin-right: 10%;
    }
}
</style>
""", unsafe_allow_html=True)

# T·∫£i c√°c bi·∫øn m√¥i tr∆∞·ªùng t·ª´ file .env
load_dotenv()

def stream_parser(stream):
    """
    L·∫Øng nghe m·ªôt stream t·ª´ LangChain v√† ch·ªâ yield (ƒë·∫©y ra) ph·∫ßn n·ªôi dung.
    """
    for chunk in stream:
        # V·ªõi document_chain, chunk l√† string tr·ª±c ti·∫øp
        if isinstance(chunk, str):
            yield chunk
        # V·ªõi ConversationalRetrievalChain, chunk c√≥ key "answer"
        elif isinstance(chunk, dict) and "answer" in chunk:
            yield chunk["answer"]

def run_async_in_thread(async_func, *args):
    """
    Ch·∫°y h√†m async trong thread ri√™ng ƒë·ªÉ tr√°nh l·ªói event loop.
    """
    def run_in_thread():
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            return loop.run_until_complete(async_func(*args))
        finally:
            loop.close()
    
    thread = threading.Thread(target=run_in_thread)
    thread.start()
    thread.join()
    return thread.result if hasattr(thread, 'result') else None

def safe_stream_qa(qa_chain, prompt):
    """
    Wrapper an to√†n cho streaming QA ƒë·ªÉ tr√°nh l·ªói event loop.
    """
    try:
        # Th·ª≠ streaming tr∆∞·ªõc
        return qa_chain.stream(prompt)
    except Exception as e:
        if "event loop" in str(e).lower():
            # N·∫øu g·∫∑p l·ªói event loop, d√πng invoke thay th·∫ø
            st.warning("ƒêang s·ª≠ d·ª•ng ch·∫ø ƒë·ªô ƒë·ªìng b·ªô do gi·ªõi h·∫°n k·ªπ thu·∫≠t...")
            result = qa_chain.invoke(prompt)
            # T·∫°o generator gi·∫£ ƒë·ªÉ t∆∞∆°ng th√≠ch v·ªõi stream_parser
            def fake_stream():
                yield {"answer": result.get("answer", "")}
            return fake_stream()
        else:
            raise e

# --- Kh·ªüi t·∫°o v·ªõi x·ª≠ l√Ω event loop ---
def initialize_components():
    """Kh·ªüi t·∫°o c√°c th√†nh ph·∫ßn v·ªõi x·ª≠ l√Ω event loop an to√†n."""
    try:
        # ƒê·∫£m b·∫£o c√≥ event loop cho thread hi·ªán t·∫°i
        try:
            loop = asyncio.get_event_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
        
        google_api_key = os.getenv("GOOGLE_API_KEY")
        cohere_api_key = os.getenv("COHERE_API_KEY")

        if not cohere_api_key:
            st.error("Vui l√≤ng thi·∫øt l·∫≠p bi·∫øn m√¥i tr∆∞·ªùng COHERE_API_KEY trong file .env")
            st.stop()
        #cohere.configure(api_key=cohere_api_key)
        if not google_api_key:
            st.error("Vui l√≤ng thi·∫øt l·∫≠p bi·∫øn m√¥i tr∆∞·ªùng GOOGLE_API_KEY trong file .env")
            st.stop()
        genai.configure(api_key=google_api_key)

        supabase_url = os.getenv("SUPABASE_URL")
        supabase_key = os.getenv("SUPABASE_KEY")
        if not all([supabase_url, supabase_key]):
            st.error("Vui l√≤ng thi·∫øt l·∫≠p c√°c bi·∫øn m√¥i tr∆∞·ªùng SUPABASE_URL, v√† SUPABASE_KEY trong file .env")
            st.stop()

        supabase: Client = create_client(supabase_url, supabase_key)

        # Kh·ªüi t·∫°o embeddings v·ªõi x·ª≠ l√Ω l·ªói
        embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
        
        vector_store = SupabaseVectorStore(
            client=supabase, 
            table_name="documents", 
            embedding=embeddings, 
            query_name="match_documents"
        )
        
        # Kh·ªüi t·∫°o LLM
        llm = ChatGoogleGenerativeAI(
            model="gemini-1.5-flash", 
            temperature=1.0, 
           # convert_system_message_to_human=True
        )
        
        return supabase, embeddings, vector_store, llm
        
    except Exception as e:
        st.error(f"L·ªói kh·ªüi t·∫°o: {e}")
        st.stop()

# Kh·ªüi t·∫°o components
try:
    supabase, embeddings, vector_store, llm = initialize_components()
    
    # Kh·ªüi t·∫°o Authentication
    auth_manager = AuthManager(supabase)
    auth_ui = AuthUI(auth_manager)
    
    # Kh·ªüi t·∫°o Re-Ranker
    reranker = CohereRerank(
        model="rerank-multilingual-v3.0",
        top_n=5
    )
    
    # Kh·ªüi t·∫°o Semantic Chunking Manager
    chunking_manager = ChunkingManager(embeddings)
    # Thi·∫øt l·∫≠p prompt
    prompt_template = ("""
                       B·∫°n l√† m·ªôt tr·ª£ gi·∫£ng AI chuy√™n ng√†nh cho sinh vi√™n ƒë·∫°i h·ªçc, th√¢n thi·ªán v√† c·ª±c k·ª≥ c·∫©n th·∫≠n. Nhi·ªám v·ª• c·ªßa b·∫°n l√† gi√∫p sinh vi√™n hi·ªÉu s√¢u c√°c kh√°i ni·ªám, gi·∫£i b√†i t·∫≠p v√† √¥n t·∫≠p d·ª±a tr√™n t√†i li·ªáu h·ªçc t·∫≠p c·ªßa h·ªç.

                       NHI·ªÜM V·ª§: D·ª±a v√†o d·ªØ li·ªáu trong t√†i li·ªáu, tr·∫£ l·ªùi tr·ª±c ti·∫øp, n·∫øu kh√¥ng bi·∫øt th√¨ n√≥i 'kh√¥ng c√≥ trong d·ªØ li·ªáu'

                       H∆Ø·ªöNG D·∫™N:
                       1. ƒê·ªçc k·ªπ c√¢u h·ªèi
                       2. N·∫øu th√¥ng tin c√≥ trong t√†i li·ªáu: Tr·∫£ l·ªùi chi ti·∫øt, r√µ r√†ng
                       3. N·∫øu kh√¥ng c√≥ th√¥ng tin: N√≥i r·∫±ng "T√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin n√†y trong t√†i li·ªáu ƒë√£ cung c·∫•p"
                       4. S·ª≠ d·ª•ng Markdown ƒë·ªÉ format ƒë·∫πp
                       
                       Ng·ªØ c·∫£nh t√†i li·ªáu:
                       {context}

                       C√¢u h·ªèi: {question}
                       
                       L·ªãch s·ª≠ tr√≤ chuy·ªán:
                       {chat_history}

                       Tr·∫£ l·ªùi:""")
    
    QA_prompt = PromptTemplate.from_template(prompt_template)
                        
    # Thi·∫øt l·∫≠p b·ªô nh·ªõ ƒë·ªÉ l∆∞u tr·ªØ l·ªãch s·ª≠ cu·ªôc tr√≤ chuy·ªán
    memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True, output_key="answer")

    # Chain nh·ªè ƒë·ªÉ k·∫øt h·ª£p t√†i li·ªáu v√†o prompt
    document_chain = create_stuff_documents_chain(llm, QA_prompt)

    # Chain l·ªõn h∆°n
    #conversation_retrieval_chain = create_retrieval_chain(retriever=vector_store.as_retriever(search_kwargs={"k": 7}), document_chain=document_chain)

    # T·∫°o chu·ªói x·ª≠ l√Ω h·ªôi tho·∫°i
    #qa = ConversationalRetrievalChain.from_llm(llm, retriever=vector_store.as_retriever(search_kwargs={"k": 7}), memory=memory, combine_docs_chain_kwargs={"prompt": QA_prompt})
    #qa = conversation_retrieval_chain
    retriever = vector_store.as_retriever(search_kwargs={"k": 5, "score_threshold": 0.5})
except Exception as e:
    st.error(f"L·ªói kh·ªüi t·∫°o: {e}")
    st.stop()


def process_document(uploaded_file, chunking_strategy="adaptive"):
    """
    H√†m x·ª≠ l√Ω file ƒë∆∞·ª£c t·∫£i l√™n: ƒë·ªçc, c·∫Øt nh·ªè th√¥ng minh v√† l∆∞u embeddings v√†o Supabase.
    """
    try:
        # T·∫°o th∆∞ m·ª•c temp n·∫øu ch∆∞a c√≥
        temp_dir = "./temp_files"
        os.makedirs(temp_dir, exist_ok=True)
        
        temp_file_path = os.path.join(temp_dir, uploaded_file.name)
        
        with open(temp_file_path, "wb") as f:
            f.write(uploaded_file.getbuffer())

        if uploaded_file.type == "application/pdf":
            loader = PyPDFLoader(temp_file_path)
        elif uploaded_file.type == "application/vnd.openxmlformats-officedocument.wordprocessingml.document":
            loader = Docx2txtLoader(temp_file_path)
        else:
            loader = TextLoader(temp_file_path)
        
        documents = loader.load()
        for doc in documents:
            doc.page_content = doc.page_content.replace('\u0000', '')

        # S·ª≠ d·ª•ng Semantic Chunking thay v√¨ basic chunking
        st.write(f"üß† **ƒêang √°p d·ª•ng {chunking_strategy} chunking...**")
        docs, chunk_stats = chunking_manager.process_documents(documents, chunking_strategy)
        
        # Hi·ªÉn th·ªã th·ªëng k√™ chunking chi ti·∫øt
        st.write(f"üìä **Th·ªëng k√™ chunking:**")
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("T·ªïng chunks", chunk_stats.get('total_chunks', 0))
        with col2:
            st.metric("ƒê·ªô d√†i TB", f"{chunk_stats.get('avg_chunk_length', 0):.0f}")
        with col3:
            st.metric("Th·ªùi gian x·ª≠ l√Ω", f"{chunk_stats.get('processing_time', 0):.2f}s")
        
        # Hi·ªÉn th·ªã ph√¢n b·ªë lo·∫°i chunks
        chunk_types = chunk_stats.get('chunk_types', {})
        if chunk_types:
            st.write("üìã **Ph√¢n b·ªë lo·∫°i chunks:**")
            for chunk_type, count in chunk_types.items():
                st.write(f"‚Ä¢ {chunk_type}: {count} chunks")
        
        # Add metadata to documents
        for doc in docs:
            doc.metadata["source_file"] = uploaded_file.name
            doc.metadata["upload_time"] = time.time()
            doc.metadata["chunking_strategy"] = chunking_strategy
        
        vector_store.add_documents(docs)
        st.write(f"‚úÖ **ƒê√£ l∆∞u {len(docs)} chunks v√†o vector database**")

        os.remove(temp_file_path)
        return True, chunk_stats
    except Exception as e:
        st.error(f"L·ªói khi x·ª≠ l√Ω t√†i li·ªáu: {e}")
        return False, {}

# ===== HELPER FUNCTIONS =====
def display_typing_animation():
    """Hi·ªÉn th·ªã animation typing v·ªõi emoji"""
    typing_placeholder = st.empty()
    for i in range(3):
        typing_placeholder.markdown(f"ü§ñ **Tr·ª£ l√Ω AI ƒëang suy nghƒ©{'.' * (i + 1)}**")
        time.sleep(0.5)
    typing_placeholder.empty()

def get_file_stats():
    """L·∫•y th·ªëng k√™ file ƒë√£ upload"""
    if "uploaded_files_count" not in st.session_state:
        st.session_state.uploaded_files_count = 0
    if "total_messages" not in st.session_state:
        st.session_state.total_messages = len(st.session_state.get("messages", []))
    return st.session_state.uploaded_files_count, st.session_state.total_messages

# ===== AUTHENTICATION CHECK =====
if not auth_ui.render_auth_page():
    st.stop()

# ===== MAIN INTERFACE =====

# Beautiful Header with Animation
st.markdown("""
<div class="main-header">
    <h1 class="main-title">üß† AI Tr·ª£ Gi·∫£ng To√°n Tin</h1>
    <p class="main-subtitle">‚ú® Powered by Gemini 2.5 Pro & Supabase Vector Store ‚ú®</p>
    <div style="margin-top: 1rem;">
        <span style="background: rgba(255,255,255,0.2); padding: 0.5rem 1rem; border-radius: 20px; margin: 0 0.5rem; font-size: 0.9rem;">üìö H·ªèi ƒë√°p th√¥ng minh</span>
        <span style="background: rgba(255,255,255,0.2); padding: 0.5rem 1rem; border-radius: 20px; margin: 0 0.5rem; font-size: 0.9rem;">üöÄ Streaming Response</span>
        <span style="background: rgba(255,255,255,0.2); padding: 0.5rem 1rem; border-radius: 20px; margin: 0 0.5rem; font-size: 0.9rem;">üéØ Context-Aware</span>
    </div>
</div>
""", unsafe_allow_html=True)

# Stats Dashboard
file_count, message_count = get_file_stats()
col1, col2, col3, col4 = st.columns(4)

with col1:
    st.markdown(f"""
    <div class="stats-card">
        <div class="stats-number">{file_count}</div>
        <div class="stats-label">üìÑ T√†i li·ªáu</div>
    </div>
    """, unsafe_allow_html=True)

with col2:
    st.markdown(f"""
    <div class="stats-card">
        <div class="stats-number">{message_count}</div>
        <div class="stats-label">üí¨ Tin nh·∫Øn</div>
    </div>
    """, unsafe_allow_html=True)

with col3:
    st.markdown("""
    <div class="stats-card">
        <div class="stats-number">üî•</div>
        <div class="stats-label">‚ö° Gemini 2.5</div>
    </div>
    """, unsafe_allow_html=True)

with col4:
    st.markdown("""
    <div class="stats-card">
        <div class="stats-number">‚ú®</div>
        <div class="stats-label">üé® Modern UI</div>
    </div>
    """, unsafe_allow_html=True)

# ===== ENHANCED SIDEBAR =====
with st.sidebar:
    # User Profile Section
    auth_ui.render_user_profile()
    
    st.markdown("---")
    
    # Sidebar Header
    st.markdown("""
    <div class="sidebar-header">
        <h2 style="margin: 0; font-size: 1.5rem;">üìö Qu·∫£n l√Ω T√†i li·ªáu</h2>
        <p style="margin: 0.5rem 0 0 0; opacity: 0.9;">Upload & x·ª≠ l√Ω t√†i li·ªáu h·ªçc t·∫≠p</p>
    </div>
    """, unsafe_allow_html=True)
    
    # File Upload Section - Admin Only
    if auth_manager.can_upload_documents():
        st.markdown("### üìÅ T·∫£i l√™n t√†i li·ªáu")
        uploaded_files = st.file_uploader(
            "K√©o th·∫£ ho·∫∑c ch·ªçn file",
            type=["pdf", "docx", "txt"],
            accept_multiple_files=True,
            help="H·ªó tr·ª£: PDF, DOCX, TXT. C√≥ th·ªÉ ch·ªçn nhi·ªÅu file c√πng l√∫c."
        )
        
        # Chunking Strategy Selection
        st.markdown("### üß† Chi·∫øn l∆∞·ª£c c·∫Øt nh·ªè d·ªØ li·ªáu")
        strategies = chunking_manager.get_available_strategies()
        strategy_names = list(strategies.keys())
        strategy_descriptions = [strategies[key] for key in strategy_names]
        
        selected_strategy = st.selectbox(
            "Ch·ªçn ph∆∞∆°ng ph√°p chunking:",
            options=strategy_names,
            index=1,  # Default to 'adaptive'
            format_func=lambda x: f"{x.title()} - {strategies[x]}",
            help="Adaptive s·∫Ω t·ª± ƒë·ªông ch·ªçn ph∆∞∆°ng ph√°p t·ªët nh·∫•t cho t·ª´ng lo·∫°i n·ªôi dung"
        )
        
        # Show strategy description
        st.info(f"üìù **{selected_strategy.title()}**: {strategies[selected_strategy]}")
        
    else:
        st.markdown("### üîí T·∫£i l√™n t√†i li·ªáu")
        st.info("‚ö†Ô∏è Ch·ªâ c√≥ qu·∫£n tr·ªã vi√™n m·ªõi c√≥ th·ªÉ t·∫£i l√™n t√†i li·ªáu.")
        uploaded_files = None
        selected_strategy = "adaptive"
    
    if uploaded_files:
        st.markdown(f"**üìä ƒê√£ ch·ªçn {len(uploaded_files)} file:**")
        for file in uploaded_files:
            file_size = len(file.getvalue()) / 1024  # KB
            st.markdown(f"‚Ä¢ `{file.name}` ({file_size:.1f} KB)")
        
        st.markdown("---")
        
        if st.button("üöÄ X·ª≠ l√Ω v√† N·∫°p ki·∫øn th·ª©c", use_container_width=True):
            progress_bar = st.progress(0)
            status_text = st.empty()
            total_stats = {'total_chunks': 0, 'total_time': 0, 'strategies_used': []}
            
            for i, file in enumerate(uploaded_files):
                status_text.text(f"ƒêang x·ª≠ l√Ω: {file.name} v·ªõi {selected_strategy} chunking")
                progress_bar.progress((i + 1) / len(uploaded_files))
                
                success, chunk_stats = process_document(file, selected_strategy)
                if success:
                    st.session_state.uploaded_files_count += 1
                    total_stats['total_chunks'] += chunk_stats.get('total_chunks', 0)
                    total_stats['total_time'] += chunk_stats.get('processing_time', 0)
                    total_stats['strategies_used'].append(chunk_stats.get('strategy_used', selected_strategy))
                    
            progress_bar.empty()
            status_text.empty()
            
            # Show summary statistics
            st.success(f"‚úÖ ƒê√£ x·ª≠ l√Ω th√†nh c√¥ng {len(uploaded_files)} t√†i li·ªáu!")
            
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("üìÑ T·ªïng chunks", total_stats['total_chunks'])
            with col2:
                st.metric("‚è±Ô∏è T·ªïng th·ªùi gian", f"{total_stats['total_time']:.2f}s")
            with col3:
                st.metric("üß† Chi·∫øn l∆∞·ª£c", selected_strategy.title())
            
            st.balloons()
    
    # Divider
    st.markdown("---")
    
    # Quick Actions
    st.markdown("### ‚ö° Thao t√°c nhanh")
    
    col1, col2 = st.columns(2)
    with col1:
        if st.button("üóëÔ∏è X√≥a chat", use_container_width=True):
            st.session_state.messages = []
            st.rerun()
    
    with col2:
        if st.button("üîÑ Reset", use_container_width=True):
            for key in list(st.session_state.keys()):
                del st.session_state[key]
            st.rerun()
    
    # Tips Section
    st.markdown("---")
    st.markdown("### üí° M·∫πo s·ª≠ d·ª•ng")
    st.markdown("""
    ‚Ä¢ üìù **H·ªèi c·ª• th·ªÉ**: "Gi·∫£i th√≠ch thu·∫≠t to√°n Dijkstra"
    ‚Ä¢ üîç **T√¨m ki·∫øm**: "T√¨m ƒë·ªãnh nghƒ©a v·ªÅ ƒë·ªì th·ªã"
    ‚Ä¢ üìä **So s√°nh**: "So s√°nh BFS v√† DFS"
    ‚Ä¢ üßÆ **B√†i t·∫≠p**: "Cho v√≠ d·ª• v·ªÅ c√¢y nh·ªã ph√¢n"
    """)
    
    # Footer
    st.markdown("---")
    st.markdown("""
    <div style="text-align: center; opacity: 0.7; font-size: 0.8rem;">
        <p>ü§ñ Made with ‚ù§Ô∏è by Duy</p>
        <p>Powered by Streamlit & Gemini</p>
    </div>
    """, unsafe_allow_html=True)

# ===== ENHANCED CHAT INTERFACE =====

# Initialize session state
if "messages" not in st.session_state:
    st.session_state.messages = []

# Welcome message for first time users
if not st.session_state.messages:
    st.markdown("""
    <div style="text-align: center; padding: 2rem; background: rgba(255,255,255,0.1); border-radius: 15px; margin: 2rem 0; backdrop-filter: blur(10px);">
        <h3 style="color: #4ECDC4; margin-bottom: 1rem;">üëã Ch√†o m·ª´ng ƒë·∫øn v·ªõi AI Tr·ª£ Gi·∫£ng!</h3>
        <p style="color: rgba(255,255,255,0.8); margin-bottom: 1rem;">T√¥i l√† tr·ª£ l√Ω AI chuy√™n ng√†nh To√°n Tin, s·∫µn s√†ng gi√∫p b·∫°n:</p>
        <div style="display: flex; justify-content: center; flex-wrap: wrap; gap: 1rem; margin-top: 1rem;">
            <span style="background: rgba(255,107,107,0.2); color: #FF6B6B; padding: 0.5rem 1rem; border-radius: 20px; font-size: 0.9rem;">üßÆ Gi·∫£i b√†i t·∫≠p</span>
            <span style="background: rgba(78,205,196,0.2); color: #4ECDC4; padding: 0.5rem 1rem; border-radius: 20px; font-size: 0.9rem;">üìö Gi·∫£i th√≠ch l√Ω thuy·∫øt</span>
            <span style="background: rgba(69,183,209,0.2); color: #45B7D1; padding: 0.5rem 1rem; border-radius: 20px; font-size: 0.9rem;">üîç T√¨m ki·∫øm th√¥ng tin</span>
        </div>
        <p style="color: rgba(255,255,255,0.6); font-size: 0.9rem; margin-top: 1rem;">üí° H√£y upload t√†i li·ªáu v√† b·∫Øt ƒë·∫ßu h·ªèi ƒë√°p!</p>
    </div>
    """, unsafe_allow_html=True)

# Display chat messages with enhanced styling
for i, message in enumerate(st.session_state.messages):
    with st.chat_message(message["role"], avatar="üë§" if message["role"] == "user" else "ü§ñ"):
        if message["role"] == "user":
            st.markdown(f"**B·∫°n:** {message['content']}")
        else:
            st.markdown(message["content"])

# Enhanced chat input with suggestions
if prompt := st.chat_input("üí¨ H√£y ƒë·∫∑t c√¢u h·ªèi v·ªÅ t√†i li·ªáu c·ªßa b·∫°n...", key="chat_input"):
    chat_history = memory.load_memory_variables({}).get("chat_history", [])

    # Add user message to history
    st.session_state.messages.append({"role": "user", "content": prompt})
    st.session_state.total_messages = len(st.session_state.messages)
    
    # Display user message
    with st.chat_message("user", avatar="üë§"):
        st.markdown(f"**B·∫°n:** {prompt}")

    # Display assistant response with enhanced streaming
    with st.chat_message("assistant", avatar="ü§ñ"):
        # Show typing indicator
        thinking_placeholder = st.empty()
        thinking_placeholder.markdown("ü§ñ **ƒêang ph√¢n t√≠ch t√†i li·ªáu v√† t·∫°o c√¢u tr·∫£ l·ªùi...** ‚è≥")
        
        try:
            # Debug: Show retrieval process
            st.write("üîç **Debug - ƒêang t√¨m ki·∫øm t√†i li·ªáu...**")
            retrieved_docs = retriever.invoke(prompt)
            st.write(f"üìÑ **T√¨m th·∫•y {len(retrieved_docs)} t√†i li·ªáu li√™n quan**")
            
            # Show chunking strategy info for retrieved docs
            chunk_strategies = {}
            for doc in retrieved_docs:
                strategy = doc.metadata.get('chunking_strategy', 'unknown')
                chunk_strategies[strategy] = chunk_strategies.get(strategy, 0) + 1
            
            if chunk_strategies:
                strategy_info = ", ".join([f"{k}: {v}" for k, v in chunk_strategies.items()])
                st.write(f"üß† **Chunking strategies: {strategy_info}**")
            
            # Show retrieved documents for debugging
            # with st.expander("üìã Xem t√†i li·ªáu ƒë∆∞·ª£c t√¨m th·∫•y"):
            #     for i, doc in enumerate(retrieved_docs):
            #         st.write(f"**T√†i li·ªáu {i+1}:**")
            #         st.write(doc.page_content[:300] + "..." if len(doc.page_content) > 300 else doc.page_content)
            #         st.write("---")

            st.write("üéØ **ƒêang rerank t√†i li·ªáu...**")
            reranked_docs = reranker.compress_documents(
                documents=retrieved_docs,
                query=prompt
            )
            st.write(f"‚ú® **Sau rerank: {len(reranked_docs)} t√†i li·ªáu t·ªët nh·∫•t**")
            
            # Show final context being sent to LLM
            # with st.expander("üìù Xem context cu·ªëi c√πng g·ª≠i cho AI"):
            #     context_text = "\n\n".join([doc.page_content for doc in reranked_docs])
            #     st.write(f"**T·ªïng ƒë·ªô d√†i context:** {len(context_text)} k√Ω t·ª±")
            #     st.text_area("Context:", context_text, height=200)
            
            # Show reranked documents for debugging
            # with st.expander("üèÜ Xem t√†i li·ªáu sau rerank"):
            #     for i, doc in enumerate(reranked_docs):
            #         st.write(f"**Top {i+1}:**")
            #         st.write(doc.page_content[:300] + "..." if len(doc.page_content) > 300 else doc.page_content)
            #         st.write("---")

            st.write("ü§ñ **ƒêang t·∫°o c√¢u tr·∫£ l·ªùi...**")
            
            # Stream the response using safe wrapper
            response_stream = document_chain.stream({
                "question": prompt,
                "context": reranked_docs,
                "chat_history": chat_history
            })
            thinking_placeholder.empty()
            
            # Display streaming response with better formatting
            full_response = st.write_stream(stream_parser(response_stream))
            
            # Add reaction buttons
            col1, col2, col3, col4 = st.columns([1, 1, 1, 6])
            with col1:
                if st.button("üëç", key=f"like_{len(st.session_state.messages)}"):
                    st.toast("C·∫£m ∆°n ph·∫£n h·ªìi c·ªßa b·∫°n! üòä", icon="üëç")
            with col2:
                if st.button("üëé", key=f"dislike_{len(st.session_state.messages)}"):
                    st.toast("T√¥i s·∫Ω c·ªë g·∫Øng c·∫£i thi·ªán! üôè", icon="üëé")
            with col3:
                if st.button("üîÑ", key=f"retry_{len(st.session_state.messages)}"):
                    st.rerun()
                    
        except Exception as e:
            thinking_placeholder.empty()
            st.error(f"‚ùå C√≥ l·ªói x·∫£y ra: {str(e)}")
            full_response = "Xin l·ªói, t√¥i g·∫∑p s·ª± c·ªë khi x·ª≠ l√Ω c√¢u h·ªèi c·ªßa b·∫°n. Vui l√≤ng th·ª≠ l·∫°i."
        
    # Add response to history and memory
    st.session_state.messages.append({"role": "assistant", "content": full_response})
    st.session_state.total_messages = len(st.session_state.messages)
    
    # Update memory with the conversation
    memory.save_context({"input": prompt}, {"answer": full_response})

# Quick question suggestions
if not st.session_state.messages or len(st.session_state.messages) < 2:
    st.markdown("### üí° C√¢u h·ªèi g·ª£i √Ω:")
    
    col1, col2 = st.columns(2)
    
    with col1:
        if st.button("üîç T√¨m ƒë·ªãnh nghƒ©a c∆° b·∫£n", use_container_width=True):
            st.session_state.suggested_question = "H√£y gi·∫£i th√≠ch c√°c ƒë·ªãnh nghƒ©a c∆° b·∫£n trong t√†i li·ªáu"
            st.rerun()
            
        if st.button("üìä So s√°nh c√°c kh√°i ni·ªám", use_container_width=True):
            st.session_state.suggested_question = "So s√°nh c√°c kh√°i ni·ªám ch√≠nh trong t√†i li·ªáu"
            st.rerun()
    
    with col2:
        if st.button("üßÆ Gi·∫£i th√≠ch thu·∫≠t to√°n", use_container_width=True):
            st.session_state.suggested_question = "Gi·∫£i th√≠ch c√°c thu·∫≠t to√°n ƒë∆∞·ª£c ƒë·ªÅ c·∫≠p trong t√†i li·ªáu"
            st.rerun()
            
        if st.button("üìù T√≥m t·∫Øt n·ªôi dung", use_container_width=True):
            st.session_state.suggested_question = "T√≥m t·∫Øt nh·ªØng ƒëi·ªÉm ch√≠nh trong t√†i li·ªáu"
            st.rerun()
